# Latent Dirichlet Allocation

## Overview
latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics

## Bag-of-words v/s LDA

Consider the below BOW model where we have documents *d*, topics *t* and probability that given the document *d*, the word is *t* as `P(t|d)`. Now if we compute the number of parameters in this model (C-BOW), we get **500 times 1000 = 500,000 parameters.**<br><br>

<img src="./images/1. C-BOW.png" height="200"></img>

This is too many parameters to figure out. Another way to appraoch this problem is by adding a notion of small set of *topics or latent variables* to our model. So in the above example, every document is associated with underlying mixture of topics. In the refactored example below, we see 2 sets of probabilities :<br>

1. Probability of a particular topic *z*, given the document *d* : `P(z|d)`
2. Probability of a particular term *t*, given the topix *z* : `P(t|z)`

The final probability of a term given a document (`P(t|d)`) will be the sum of above mentioned probabilities.

<img src="./images/2. Latent Variables.png" height="200"></img>

Now using LDA, the number of parameters will be **(500 (documents) times 10 (topics) = 5000) + (10 (topics) times 1000 (terms)) = 15,000 parameters** which is significantly less than C-BOW.

<img src="./images/4. Matrix multiplcation max.png" height="200"></img>

## Matrix Multiplication 

### Matrix : Bag-of-Words

Here, we have 5 sets of documents and 7 topics and we have pre-processed every document to include only important words. Now we calculate the occurence of each word in the document and put them in coressponding row and to calculate the probability we divide them by total no. of words.<br>

<img src="./images/5. BOW-Matrix.png" height="200"></img>

### Matrix : Document
Here, we have 5 sets of documents and say 3 sets of topics. Consider an example of document no. 3 which is combination of 70% science, 10% politics and 20% sports. That's it, we save these values in the matrix to form our *document matrix.*<br>

<img src="./images/6. document matrix.png" height="200"></img>

### Matrix : Topic
Here lets say, we have 3 topics and 6 random words and the probability that word generated by the topic *politics*  is as given below. Now we feed these probabilities into matrix to form our *topic matrix.*<br>

<img src="./images/7. topic matrix - 1.png" height="160"></img>
<img src="./images/6. topic matrix -2.png" height="150"></img>


## Distributions

### Beta Distributions
Consider the following scenario where a coin is tossed and it gives heads : a times and tails : b times, then the Beta distribution is given according to the gamma rule as below -<br>

<img src="./images/8. Beta Distribution .png" height="200"></img><br><br>

The gamma function is defined as follows -<br><br>

<img src="./images/9. gamma function.png" height="40"></img><br><br>

The advantage of Gamma function is that we can use gamma functions for decimals as well.<br>

<img src="./images/10. decimal beta distribution .png" height="200"></img><br><br>

### Dirichlet Distributions 

Consider an example where a news article is comprised of 60% politics , 30% science and 10% sports news. Then the coressponding Dirichlet distribution is given by - <br>
<img src="./images/12. Dirichlet distribution .png" height="250"></img><br><br>

The coressponding 3-D distribution would be similar to the diagram below. *If we want a good topic model we need to pick small parameters like the one on the left.*<br><br><img src="./images/13. 3D D Distributions.png" height="200"></img><br><br>

## LDA Model

### LDA Model : Sample a Topic

Consider a case where we pick a point in Dirichlet distribution that coressponds to a news article that contains 80% percent politics, 10% science and 10% sports as below : 
<br><br><img src="./images/16. sample a topic - 1.png" height="200"></img><br><br>

Now we sample multiple documents from Dirichlet distribution and merge all the vectors to create our first matrix : *A matrix that indices documents with their corresponding topics.*.<br>
<br><br><img src="./images/15. sample a topic - 2.png" height="200"></img>
<img src="./images/14. Sample a topic-3.png" height="200"></img><br><br>

### LDA Model : Sample a Word

Consider a news article which consists of the following words : *space, climate, vote, rule* and we draw its corresponding Dirichlet distribution as below:
<br><br><img src="./images/16. sample a word-1.png" height="200"></img><br><br>

Similarly, we sample multiple topics and create a probabilistic distribution for them.
<br><br><img src="./images/17. sample a word -2.png" height="200"></img><br><br>

Later, we combine the vectors to form a matrix, from this matrix we can say that since document 1 has a 40% probability of containing the word *space* and 40% probability of containing the word *climate*, document 1 might be about the topic *science*.

<br><br><img src="./images/18. sample a word - 3.png" height="200"></img><br><br>












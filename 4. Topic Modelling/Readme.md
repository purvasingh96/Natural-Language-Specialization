# Latent Dirichlet Allocation

## Overview
latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics

## Bag-of-words v/s LDA

Consider the below BOW model where we have documents *d*, topics *t* and probability that given the document *d*, the word is *t* as `P(t|d)`. Now if we compute the number of parameters in this model (C-BOW), we get **500 times 1000 = 500,000 parameters.**<br><br>

<img src="./images/1. C-BOW.png" height="200"></img>

This is too many parameters to figure out. Another way to appraoch this problem is by adding a notion of small set of *topics or latent variables* to our model. So in the above example, every document is associated with underlying mixture of topics. In the refactored example below, we see 2 sets of probabilities :<br>

1. Probability of a particular topic *z*, given the document *d* : `P(z|d)`
2. Probability of a particular term *t*, given the topix *z* : `P(t|z)`

The final probability of a term given a document (`P(t|d)`) will be the sum of above mentioned probabilities.

<img src="./images/2. Latent Variables.png" height="200"></img>

Now using LDA, the number of parameters will be **(500 (documents) times 10 (topics) = 5000) + (10 (topics) times 1000 (terms)) = 15,000 parameters** which is significantly less than C-BOW.

<img src="./images/4. Matrix multiplcation max.png" height="200"></img>

## Matrix Multiplication 

### Matrix : Bag-of-Words

Here, we have 5 sets of documents and 7 topics and we have pre-processed every document to include only important words. Now we calculate the occurence of each word in the document and put them in coressponding row and to calculate the probability we divide them by total no. of words.<br>

<img src="./images/5. BOW-Matrix.png" height="200"></img>

### Matrix : Document
Here, we have 5 sets of documents and say 3 sets of topics. Consider an example of document no. 3 which is combination of 70% science, 10% politics and 20% sports. That's it, we save these values in the matrix to form our *document matrix.*<br>

<img src="./images/6. document matrix.png" height="200"></img>

### Matrix : Topic
Here lets say, we have 3 topics and 6 random words and the probability that word generated by the topic *politics*  is as given below. Now we feed these probabilities into matrix to form our *topic matrix.*<br>

<img src="./images/7. topic matrix - 1.png" height="160"></img>
<img src="./images/6. topic matrix -2.png" height="150"></img>


## Distributions

## Beta Distributions
Consider the following scenario where a coin is tossed and it gives heads : a times and tails : b times, then the Beta distribution is given according to the gamma rule as below -<br>

<img src="./images/8. Beta Distribution .png" height="200"></img><br><br>

The gamma function is defined as follows -<br><br>

<img src="./images/9. gamma function.png" height="40"></img><br><br>

The advantage of Gamma function is that we can use gamma functions for decimals as well.<br>

<img src="./images/10. decimal beta distribution .png" height="200"></img><br><br>

## Dirichlet Distributions 





